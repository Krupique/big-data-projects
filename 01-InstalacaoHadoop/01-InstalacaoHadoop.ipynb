{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como instalar e configurar o Hadoop no CentOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fala pessoal, beleza?\n",
    "\n",
    "Neste artigo vou demonstrar como instalar e configurar o Hadoop no sistema operacional CentOS que está rodando sobre uma VM no VirtualBox. Caso você tenha interesse em entender mais sobre o que é uma máquina virtual, como instalar o Ubuntu e o CentOS, vou deixar o link para tutoriais referentes a cada um deles:\n",
    "\n",
    "* <a target=\"blank\" href=\"https://medium.com/@krupck/como-instalar-e-configurar-linux-em-uma-máquina-virtual-parte-1-3-3326e24ab01b\">O que são máquinas virtuais e como instalar o VirtualBox em sua máquina</a>\n",
    "* <a target=\"blank\" href=\"https://medium.com/@krupck/como-instalar-e-configurar-linux-em-uma-máquina-virtual-parte-2-3-6e5b01050120\">Como instalar o Ubuntu na VM</a>\n",
    "* <a target=\"blank\" href=\"https://medium.com/@krupck/como-instalar-e-configurar-linux-em-uma-máquina-virtual-parte-3-3-2be8c0549847\">Como instalar o CentOS camada a camada na VM</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação do Hadoop\n",
    "\n",
    "### Utilitários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente irei instalar alguns pacotes utilitários, pois se você acompanhou as últimas postagens sobre o como instalar Linux em uma VM viu que eu instalei apenas o básico para o funcionamento do SO com interface gráfica. Então vamos lá, no terminal:\n",
    "\n",
    "> `sudo yum install bzip2 unzip rsync wget net-tools`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL (MariaDB)\n",
    "\n",
    "É recomendado ter um banco de dados no SO. Portanto irei instalar o mais simples deles, o MariaDB, que é uma versão do MySQL. No terminal:\n",
    "\n",
    "> `sudo yum install mariadb-server mariadb`\n",
    "\n",
    "Com a instalação concluída vamos inicializar o serviço de banco de dados e habilitar a sua inicialização no boot com os comandos:\n",
    "\n",
    "> `sudo systemctl start mariadb.service`<br/>\n",
    "> `sudo systemctl enable mariadb.service`<br/>\n",
    "\n",
    "Definindo a senha para o usuário root do Banco de Dados:\n",
    "\n",
    "> `mysqladmin -u root passwrod 'sua_senha'`\n",
    "\n",
    "Vamos testar a instalação iniciando o console do MySQL:\n",
    "\n",
    "> `mysql -u root -p` <br/>\n",
    "> Digite a sua senha:\n",
    "\n",
    "Se tudo deu certo é esta a tela que está aparecendo para você no terminal:\n",
    "\n",
    "<img src=\"resources/00.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que estamos no console do MariaDB, vamos testar um comando simples só para garantir que tudo está funcionando corretamente:\n",
    "\n",
    "> `SELECT user, host, password FROM mysql.user;`\n",
    "\n",
    "> Nota: Não esqueça de utilizar o \"; (ponto e vírgula)\" no final das instruções SQL aqui no terminal.\n",
    "\n",
    "<img src=\"resources/01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sair do console do MariaDB digite `exit;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação do servidor SSH\n",
    "\n",
    "SSH é a sigla para Secure Socket Shell, sendo um dos protocolos específicos de segurança de troca de arquivos entre cliente e servidor de internet, usando criptografia. O objetivo do SSH é permitir que desenvolvedores ou outros usuários realizem alterações em sites e servidores utilizando uma conexão simples e segura.\n",
    "\n",
    "No terminal:\n",
    "\n",
    "> `sudo yum install openssh-server openssh-clients`\n",
    "\n",
    "Ativando o servidor SSH no boot de inicialização:\n",
    "\n",
    "> `sudo chkconfig sshd on`\n",
    "\n",
    "Inicializando o serviço:\n",
    "\n",
    "> `sudo service sshd start`\n",
    "\n",
    "Para verificar se deu certo:\n",
    "\n",
    "> `sudo service sshd status`\n",
    "\n",
    "<img src=\"resources/02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra forma de averiguar é verificando as informações de rede na porta 22:\n",
    "\n",
    "> `sudo netstat -tulpn | grep :22`\n",
    "\n",
    "<img src=\"resources/03.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora daremos permissão para que o usuário root possa acessar livremente na porta 22. No terminal:\n",
    "\n",
    "> `sudo gedit /etc/ssh/sshd_config`\n",
    "\n",
    "No documento que foi abert, remova o # no início das linhas selecionadas:\n",
    "\n",
    "Primeira parte:<br/>\n",
    "\n",
    "<img src=\"resources/04.png\">\n",
    "\n",
    "Segunda Parte:<br/>\n",
    "> Adicione o seu usuário.<br/>\n",
    "\n",
    "<img src=\"resources/05.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinicializando o serviço ssh:\n",
    "\n",
    "> `sudo service sshd restart`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação do JRE (Java Runtime Edition)\n",
    "\n",
    "Como o Hadoop roda sobre a linguagem de programação Java, precisaremos instalar tanto o JRE quanto JDK na nossa máquina:\n",
    "\n",
    "> `sudo yum install java`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação do JDK (Java Development Kit)\n",
    "\n",
    "A mesma coisa para o JDK, mas o processo para instalação do JDK é um pouco diferente. Será necessário navegar até o site da Oracle. Você pode acessar clicando nesse link: <a target=\"_blank\" href=\"https://www.oracle.com/java/technologies/downloads/\">Oracle.com</a>\n",
    "\n",
    "<img src=\"resources/06.png\">\n",
    "\n",
    "Navegando pelo diretório do arquivo e listando conteúdo da pasta Downloads:\n",
    "\n",
    "<img src=\"resources/07.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora precisamos descompactar o arquivo e mover ele para outro local:\n",
    "\n",
    "> `tar -xzf jdk-8u202-linux-x64.tar.gz`\n",
    "\n",
    "Irei mover o jdk que foi extraído para o diretório /usr/local/hadoop_dependencies. Os diretórios usr/ e local/ já existem por padrão. Para criar o diretório hadoop_dependencies, vá em:\n",
    "> `cd /usr/local` -> Vá até o diretório usr/local/<br/>\n",
    "> `sudo mkdir hadoop_dependencies` Crie o diretório hadoop_dependencies/<br/>\n",
    "> `cd ~` Volte para o diretório raiz<br/>\n",
    "> `cd Downloads/` Navegue para o diretório Downloads/<br/>\n",
    "\n",
    "Vamos utilizar esse diretório para armazenar todos os pacotes que baixarmos durante a nossa instalação do Hadoop. Dessa forma os arquivos ficam todos organizados.\n",
    "\n",
    "> `sudo mv jdk1.8.0_202/ /usr/local/hadoop_dependencies/`\n",
    "\n",
    "Agora iremos criar um link simbólico. Entenda o link simbólico como um atalho para algum diretório ou arquivo. Portanto, quando nos referimos ao java utilizaremos o link simbólico. Dessa forma, é possível controlar as versões dos pacotes de maneira muito mais dinâmica e profissional. Pois não teremos que alterar dezenas de arquivos todas as vezes que alterarmos a versão de algum pacote.\n",
    "\n",
    "> `sudo ln -s /usr/local/hadoop_dependencies/jdk1.8.0_202/ /opt/jdk`\n",
    "\n",
    "Faremos a mesma coisa para o JRE:\n",
    "\n",
    "> `sudo ln -s /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.342.b07-1.el7_9.x86_64/jre/ /opt/jre`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando os links simbólicos:\n",
    "\n",
    "<img src=\"resources/08.png\">\n",
    "\n",
    "Perceba que o prefixo do tipo do objeto é `l` que significa link.\n",
    "\n",
    "**Adicionando variáveis de ambiente**\n",
    "\n",
    "Agora que instalamos o JRE e o JDK, precisamos definir as variáveis de ambiente para que o SO possa identifcar os pacotes que foram instalados.\n",
    "\n",
    "> `cd ~`<br/>\n",
    "> `gedit .bashrc`<br/>\n",
    "\n",
    "O arquivo .bashrc é o arquivo que armazena as variáveis de ambiente. Dentro desse arquivo adicione os valores:\n",
    "\n",
    "> export JAVA_HOME=/opt/jdk <br/>\n",
    "> export JRE_HOME=/opt/jre <br/>\n",
    "> export PATH=$PATH:$JRE_HOME/bin:$JAVA_HOME/bin <br/>\n",
    "\n",
    "<img src=\"resources/09.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para atualizar as novas configurações, digite:\n",
    "\n",
    "> `source .bashrc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desabilitando o IPv6\n",
    "\n",
    "Alguns recursos do Hadoop não funcionam muito bem com o este protocólo. Portanto, iremos desabilitar para evitar qualquer problema futuro. No terminal, vá em:\n",
    "\n",
    "> `sudo gedit /etc/sysctl.conf`<br/>\n",
    "\n",
    "No arquivo de configuração, adicione as seguintes configurações:\n",
    "\n",
    "> `net.ipv6.all.disable_ipv6 = 1`<br/>\n",
    "> `net.ipv6.default.disable_ipv6 = 1`<br/>\n",
    "> `net.ipv6.lo.disable_ipv6 = 1`<br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando o SSH para o Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente havia ativado o SSH, agora nós vamos configurar o SSH para o Hadoop. A primeira coisa que vamos fazer é gerar uma chave de segurança. O SSH é um protocólo de comunicação. No terminal:\n",
    "\n",
    "> `ssh-keygen -t rsa`\n",
    "\n",
    "Nesse momento, o SO vai perguntar aonde você quer criar essa chave. Basta digitar enter para criar no diretório sugerido. \n",
    "<img src=\"resources/10.png\">\n",
    "\n",
    "Agora, ele vai perguntar pedir para você digitar uma passphrase. É como se fosse uma senha. Não precisa digitar nada aqui, aperte Enter duas vezes e pronto.\n",
    "\n",
    "<img src=\"resources/11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos que esta chave gerada seja reconhecida e garantida pelo SSH. Portanto, devemos copiar a chave para um arquivo de chaves autorizadas:\n",
    "\n",
    "> `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys` Copiando o arquivo. <br/>\n",
    "> `chmod 0600 ~/.ssh/authorized_keys` Definindo permissões para esse arquivo. <br/>\n",
    "\n",
    "Podemos testar o ssh digitando `ssh localhost` no terminal. Digite yes e pronto, conexão estabelecidade.\n",
    "\n",
    "**Mas para que serve isso afinal?**<br/>\n",
    "Em um ambiente de cluster real, esse protocólo vai permitir que o NameNode se comunique com o DataNode através do protocólo SSH e sem a necessidade de senhas. Porque imagine só um cluster com 1000 máquinas, onde cada máquina precisa digitar a senha para se comunicar com o NameNode? Seria inviável, por isso configuramos o SSH.\n",
    "\n",
    "Para finalizar a configuração do SSH, vamos dizer quais são as máquinas que podem conectar no nosso servidor.\n",
    "\n",
    "> `sudo gedit /etc/hosts`\n",
    "\n",
    "<img src=\"resources/12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Instalação do Hadoop\n",
    "\n",
    "Agora sim estamos prontos para começar a instalar o Hadoop. A instalação do hadoop em si é bastante simples, porém, realizar todas as configurações necessárias para trabalharmos com um cluster, aí sim teremos um pouco mais de trabalho.\n",
    "\n",
    "Acesso o site oficial para downloads do Hadoop: [download dos binários](https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz)\n",
    "\n",
    "Por padrão o arquivo será baixado no diretório de Downloads. Mas você pode baixar totalmente via linha de comando no diretório atual inserindo:\n",
    "\n",
    "> `wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O arquivo foi baixado com sucesso, veja:\n",
    "\n",
    "<img src=\"resources/13.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descompactando o arquivo:\n",
    "\n",
    "> `tar -xzf hadoop-3.3.4.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos um passo a passo bem parecido com a instalação do JDK. Primeiro movemos o arquivo para o diretório /usr/local/hadoop_dependencies, depois criamos um links simbólico e configuramos as variáveis de ambiente.\n",
    "\n",
    "> `sudo mv hadoop-3.3.4 /usr/local/hadoop_dependencies/`Movendo o arquivo para outro diretório<br/>\n",
    "> `sudo ln -s /usr/local/hadoop_dependencies/hadoop-3.3.4/ /opt/hadoop` Criação do link simbólico<br/>\n",
    "\n",
    "No diretório raiz:\n",
    "> `gedit .bashrc`\n",
    "\n",
    "<img src=\"resources/20.png\">\n",
    "\n",
    "> `source .bashrc`\n",
    "\n",
    "---\n",
    "\n",
    "### Adicionando parâmetros de Configuração\n",
    "\n",
    "Temos o Hadoop instalado. Precisamos realizar as configurações em seus arquivos de configurações. Navegue até o diretório de arquivos de configuração do Hadoop:\n",
    "\n",
    "> `cd /opt/hadoop/etc/hadoop`\n",
    "\n",
    "<img src=\"resources/14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será necessário editar cinco arquivos neste diretório, são eles:\n",
    "\n",
    "\n",
    "> `gedit hadoop-env.sh`\n",
    "\n",
    "Adicione os seguintes valores:\n",
    "\n",
    "<img src=\"resources/15.png\">\n",
    "\n",
    "---\n",
    "\n",
    "> `gedit core-site.xml`\n",
    "\n",
    "Veja que o arquivo vem vazio, porque o Hadoop está no modo Standalone. Queremos colocá-lo em modo Pseudo-Distribuído. Para isso, devemos adicionar tags de configuração:\n",
    "\n",
    "<img src=\"resources/16.png\">\n",
    "\n",
    "Esse parâmetro indica que o sistema de arquivos padrão para o meu Hadoop será o HDFS e que tudo será apontado para o localhost na porta 9000.\n",
    "\n",
    "---\n",
    "\n",
    "> `gedit hdfs-site.xml`\n",
    "\n",
    "Esse parâmetro dfs.replication com o valor igual a 1 basicamente diz que não queremos replicar os blocos de arquivos para mais de um node. \n",
    "\n",
    "<img src=\"resources/17.png\">\n",
    "\n",
    "---\n",
    "\n",
    "> `gedit mapred-site.xml`\n",
    "\n",
    "Aqui eu vou informar para o Hadoop como eu quero que seja feita a gestão dos recursos. E essa gestão será feita pelo Yarn.\n",
    "\n",
    "<img src=\"resources/18.png\">\n",
    "\n",
    "---\n",
    "\n",
    "> `gedit yarn-site.xml`\n",
    "\n",
    "Um dos serviços que queremos que o Yarn gerencie é o mapreduce shuffle, etapa intermediária entre o map e o reduce.\n",
    "\n",
    "<img src=\"resources/19.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vericando instalação\n",
    "\n",
    "Depois de todas essa configurações vamos finalmever averiguar se o Hadoop está funcionando:\n",
    "\n",
    "> `hadoop version`\n",
    "\n",
    "<img src=\"resources/21.png\">\n",
    "\n",
    "Esta é a mensagem que deve aparecer para você caso o Hadoop tenha sido instalado corretamente.\n",
    "\n",
    "---\n",
    "\n",
    "Verificando se os arquivos de configuração foram instalados corretamente:\n",
    "\n",
    "> `hdfs namenode -format`\n",
    "\n",
    "<img src=\"resources/22.png\">\n",
    "\n",
    "Aqui a mensagem que queremos, indicando se ocorreu tudo bem com a inicialização do namenode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Iniciando o Hadoop\n",
    "\n",
    "Se você chegou até aqui, viu que há bastante coisas para serem configuradas até que possamos utilizar o Hadoop na prática. Tenho duas notícias:\n",
    "\n",
    "* A boa notícia: Já podemos inicializar o Hadoop e utilizar o seu sistema gerenciador de arquivos HDFS e disparar os jobs MapReduce. \n",
    "* A má notícia: Temos o mais básico do hadoop, ainda estamos na metade do caminho se quisermos utilizar os principais recursos do framework.\n",
    "\n",
    "A instalação do HBase, Hive, Zookeeper e outros recursos do Hadoop eu irei deixar para os próximos tutoriais. Agora vamos brincar um pouco com o temos até aqui.\n",
    "\n",
    "Para iniciar o Hadoop digite:\n",
    "> `start-dfs.sh`<br/>\n",
    "\n",
    "Duranta a configuração dos arquivos nós definimos que o Yarn ficaria responsável por gerenciar as operações de MapReduce do Hadoop. Então vamos iniciar o yarn:\n",
    "\n",
    "> `start-yarn.sh`<br/>\n",
    "\n",
    "Para verificar os processos java ativos, digite:\n",
    "\n",
    "> `jps`\n",
    "\n",
    "<img src=\"resources/23.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar o estado do cluster Hadoop. No teu browser navegue até o seguinte endereço: `http://localhost:8088`\n",
    "\n",
    "<img src=\"resources/24.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar o sistema de arquivos do Hadoop não há segredos. Se você sabe linux, muito provavelmente saberá utilizar o HDFS sem problemas. O Hadoop fornece dois tipos de comandos para interagir com o File System; `hadoop fs` ou `hdfs dfs`. A principal diferença é que os comandos do hadoop são compatíveis com vários sistemas de arquivos, como S3, Azure e muitos outros.\n",
    "\n",
    "\n",
    "\n",
    "Vou mostrar alguns comandos básicos, você pode utilizar o prefixo `hdfs dfs` ou `hadoop fs`:\n",
    "\n",
    "> `hdfs dfs -mkdir /nome_diretorio`: Criar diretório.<br/> \n",
    "> `hdfs dfs -rmdir /nome_diretorio`: Remover diretório.<br/>\n",
    "> `hdfs dfs -rm /nome_diretorio`: Remover arquivo.<br/>\n",
    "> `hdfs dfs -rmr /nome_diretorio`: Remove o arquivo identificado por caminho/pasta e subpastas.<br/>\n",
    "> `hdfs dfs -ls`: Exibir conteúdo do diretório<br/>\n",
    "> `hdfs dfs -put /path/file`: Faz o upload do arquivo para o hdfs.<br/>\n",
    "> `hdfs dfs -cat /path/file`: Exibe o conteúdo do arquivo.<br/>\n",
    "> `hdfs dfs -du /path/file`: Exibe o tamanho do arquivo no HDFS.<br/>\n",
    "> `hdfs dfs -dus /path/file`: Exibe o tamanho total do diretório/arquivo no HDFS.<br/>\n",
    "> `hdfs dfs -get /path/file`: Armazena arquivo/diretório do HDFS para o sistema de arquivos local.<br/>\n",
    "> `hdfs dfs -getmerge /path/file`: Armazena múltiplos arquivos/diretórios do HDFS para o sistema de arquivos local.<br/>\n",
    "> `hdfs dfs -count /path/file`: Conta o número de diretório, número de arquivos e tamanho do arquivo.<br/>\n",
    "> `hdfs dfs -mv /path/origin /path/destiny`: Move o arquivo/diretório da origem para o destino.<br/>\n",
    "> `hdfs dfs -moveFromLocal /path/origin /path/destiny`: Move arquivo/diretório do disco local para HDFS.<br/>\n",
    "> `hdfs dfs -moveToLocal /path/origin /path/destiny`: Move arquivo/diretório do HDFS para o disco local.<br/>\n",
    "> `hdfs dfs -cp /path/origin /path/destiny`: Copia o arquivo/diretório da origem para o destino.<br/>\n",
    "> `hdfs dfs -copyFromLocal /path/origin /path/destiny`: Copia o arquivo/diretório da origem local para o destino HDFS.<br/>\n",
    "> `hdfs dfs -copyToLocal /path/origin /path/destiny`: Copia o arquivo/diretório da origem HDFS para a destino local.<br/>\n",
    "> `hdfs dfs -head /path/file`: Exibe o primeiro kilobyte do arquivo.<br/>\n",
    "> `hdfs dfs -tail /path/file`: Exibe o último kilobyte do arquivo.<br/>\n",
    "> `hdfs dfs -chmod /path/file`: Altera as permissões do arquivo/diretório.<br/>\n",
    "> `hdfs dfs -find /path/file`: Procura o arquivo dentro da estrutura HDFS.<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encerrar os serviços do Hadoop e do Yarn:\n",
    "> `stop-dfs.sh`<br/>\n",
    "> `stop-yarn.sh`<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Considerações finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprender sobre o ecossistema Hadoop não é uma tarefa trivial. E na era dos serviços em Cloud abre o questionamento, vale mesmo a pena dedicar tempo e esfoço em aprender um framework que eu provavelmente nunca vou encontrar no dia-a-dia de uma empresa? \n",
    "\n",
    "Bom, se você for pesquisar a fundo vai perceber que a maioria dos serviços de Big Data em nuvens oferecidos pelas principais empresas do mercado: AWS, Microsoft e Google, são na verdade modificações do Hadoop. Então, estudando o Hadoop possibilitará você entender o cerne do Big Data e também acumular conhecimento para aprender com muito mais facilidade os serviços em nuvens oferecidos por estas empresas.  \n",
    "\n",
    "Finalizando este tutorial, eu gostaria de agradecer para quem chegou até aqui. Se gostou do conteúdo, deixa o like e se quiser pode me seguir nas redes sociais que eu pretendo sempre estar postando conteúdos relacionados a Big Data, Machine Learning e Data Science:\n",
    "\n",
    "* <a target=\"_blank\" href=\"https://github.com/krupique\">github.com/krupique</a><br/>\n",
    "* <a target=\"_blank\" href=\"https://www.linkedin.com/in/henrique-krupck/\">linkedin.com/henrique-krupck</a><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
